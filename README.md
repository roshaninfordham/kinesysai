<div align="center">

# KINESYS

### Teach robots like you teach humans ‚Äî tell it, show it, or guide it.

[![Python](https://img.shields.io/badge/Python-3.11+-3776ab?logo=python&logoColor=white)](https://www.python.org/)
[![React](https://img.shields.io/badge/React-18-61dafb?logo=react&logoColor=white)](https://react.dev/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-009688?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-3178c6?logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Tests](https://img.shields.io/badge/tests-156%20passing-brightgreen)](#testing)

**Built at Open Build Manhattan Hackathon**

</div>

---

## The Problem

Programming industrial robots today requires:
- Expensive proprietary software (ROS, vendor IDEs)
- Expert knowledge of kinematics and trajectory planning
- Hours of manual waypoint configuration per task
- Zero tolerance for error ‚Äî a wrong command can damage hardware

**Result:** robots sit idle, expensive tasks go unautomated, and non-engineers cannot participate in robot programming.

## The Solution

KINESYS is an open-source human-robot interaction platform that lets **anyone** teach a robot new tasks in three natural ways ‚Äî no robotics expertise needed:

| Mode | How you teach | What happens |
|------|--------------|--------------|
| üéôÔ∏è **Command** | Speak a natural sentence | LLM decomposes it into validated waypoints ‚Üí executes in simulation |
| üì∑ **Teach** | Demonstrate with your hands on camera | VLM watches your AR demonstration ‚Üí extracts procedure ‚Üí robot replays it |
| üñêÔ∏è **Guide** | Move your hand in front of webcam | MediaPipe maps your hand to the robot gripper in real-time ‚Üí record & replay |

Every trajectory goes through a **safety validation engine** before any motion executes. A **LangGraph state machine** guarantees the pipeline never skips validation.

---

## System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        BROWSER (Frontend)                       ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ SimCanvas    ‚îÇ  ‚îÇ  VoicePanel  ‚îÇ  ‚îÇ    TeachPanel (AR)   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Three.js     ‚îÇ  ‚îÇ  Web Speech  ‚îÇ  ‚îÇ  Webcam + MediaPipe  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ + Physics    ‚îÇ  ‚îÇ  STT / TTS   ‚îÇ  ‚îÇ  Hand landmarks      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                      ‚îÇ              ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                           ‚îÇ WebSocket (ws://localhost:8000/ws)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BACKEND (FastAPI)                            ‚îÇ
‚îÇ                           ‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                   LangGraph Pipeline                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  IDLE ‚Üí LISTENING ‚Üí DECOMPOSING ‚Üí SCENE_ANALYZING         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ       ‚Üí PLANNING ‚Üí VALIDATING ‚îÄ‚îÄ‚ñ∫ HITL? ‚îÄ‚îÄ‚ñ∫ EXECUTING     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                         ‚îÇ                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                      ERROR ‚îÄ‚îÄ‚ñ∫ IDLE (auto-recover)        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                                         ‚îÇ             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   AI Layer      ‚îÇ                    ‚îÇ   Core Engine      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ                    ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ LLMClient       ‚îÇ                    ‚îÇ ActionPrimitives   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Groq (primary) ‚îÇ                    ‚îÇ TrajectoryPlanner  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Gemini (backup)‚îÇ                    ‚îÇ SafetyValidator    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Ollama (local) ‚îÇ                    ‚îÇ StateMachine       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ                    ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ VLMClient       ‚îÇ                    ‚îÇ TrajectoryRecorder ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Ollama vision  ‚îÇ                    ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ                    ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ SceneAnalyzer   ‚îÇ                    ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ TaskDecomposer  ‚îÇ                    ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ProcExtractor   ‚îÇ                    ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Voice Command Pipeline (Command Mode)

```
User speaks
    ‚îÇ
    ‚ñº
Web Speech API (browser STT)
    ‚îÇ  transcript: "put the red cube on top of the blue cylinder"
    ‚ñº
WebSocket ‚Üí backend /ws
    ‚îÇ
    ‚ñº
LangGraph: LISTENING ‚Üí DECOMPOSING
    ‚îÇ
    ‚ñº
LLMClient.chat()  [Groq ‚Üí Gemini ‚Üí Ollama fallback chain]
    ‚îÇ  Returns JSON action plan:
    ‚îÇ  [ {action: "APPROACH", params: {target: "red_cube"}},
    ‚îÇ    {action: "GRASP",    params: {target: "red_cube"}},
    ‚îÇ    {action: "STACK",    params: {target: "blue_cylinder"}},
    ‚îÇ    {action: "RELEASE",  params: {}} ]
    ‚ñº
TrajectoryPlanner ‚Üí generates Waypoints per primitive
    ‚îÇ
    ‚ñº
SafetyValidator  (workspace bounds, velocity, collision, table)
    ‚îÇ   PASS ‚îÄ‚îÄ‚ñ∫ EXECUTING
    ‚îÇ   FAIL ‚îÄ‚îÄ‚ñ∫ ERROR with human-readable explanation
    ‚ñº
Waypoints sent back over WebSocket
    ‚îÇ
    ‚ñº
Frontend: RobotArm animates IK-solved joints
          SceneObjects update positions
          TTS narrates each step
```

---

## AR Teach Mode Pipeline

```
User opens Teach tab
    ‚îÇ
    ‚ñº
WebcamFeed activates (getUserMedia)
    ‚îÇ
    ‚ñº
MediaPipe HandLandmarker runs in browser (GPU-accelerated)
    ‚îÇ  21 landmarks per hand @ 30fps
    ‚ñº
Canvas overlay draws:
    ‚îú‚îÄ‚îÄ Table surface grid (perspective-projected)
    ‚îú‚îÄ‚îÄ Virtual scene objects (boxes, cylinders, spheres)
    ‚îÇ   ‚îî‚îÄ‚îÄ PINCH gesture near object ‚Üí "GRABBED" highlight
    ‚îî‚îÄ‚îÄ Hand skeleton (bones + joints)
    ‚îÇ
    ‚ñº
User presses Record ‚Üí teachMode.startRecording()
    ‚îÇ  Captures keyframe (video frame + palm position + gesture)
    ‚îÇ  every 500ms if hand moved > 0.02 normalized units
    ‚ñº
User presses Stop ‚Üí keyframes sent to backend
    ‚îÇ
    ‚ñº
VLMClient.analyze_images()  [Ollama: pixtral:12b / llava:13b]
    ‚îÇ  System prompt: teach_extract.txt
    ‚îÇ  Returns: { actions: [...], summary: "...", confidence: [...] }
    ‚ñº
ProcedureExtractor validates each action against PRIMITIVE_REGISTRY
    ‚îÇ  confidence < 0.7 ‚Üí flagged for human confirmation
    ‚ñº
Frontend: Review panel shows extracted steps
    ‚îÇ  User confirms / rejects individual steps
    ‚ñº
Confirmed actions ‚Üí teach_execute ‚Üí same pipeline as Command Mode
```

---

## Guide Mode Pipeline

```
User opens Guide tab
    ‚îÇ
    ‚ñº
WebcamFeed + MediaPipe (same as Teach)
    ‚îÇ
    ‚ñº
guideMode.ts maps palm position ‚Üí robot end-effector coords
    ‚îÇ  Normalized [0,1] hand ‚Üí world [-1.5, 1.5] space
    ‚îÇ  Pinch gesture ‚Üí gripper close/open
    ‚ñº
Real-time waypoints sent over WebSocket every ~100ms
    ‚îÇ
    ‚ñº
Backend: TrajectoryRecorder buffers waypoints
    ‚îÇ
    ‚ñº
User presses Stop ‚Üí recorded trajectory replayed
    ‚îÇ  Same safety validation as Command Mode
    ‚ñº
Robot arm animates the replayed trajectory
```

---

## Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend framework** | React 18 + TypeScript + Vite | UI, state management, build |
| **3D rendering** | Three.js r170 | Robot arm, scene objects, lighting |
| **Physics** | Cannon-es | Rigid body simulation, collision |
| **Styling** | Tailwind CSS | Utility-first responsive design |
| **Hand tracking** | MediaPipe Tasks Vision | 21-point hand landmark detection |
| **Speech-to-text** | Web Speech API (browser) | Continuous voice recognition |
| **Text-to-speech** | Web Speech Synthesis API | Step narration |
| **Backend framework** | FastAPI + Uvicorn | REST API + WebSocket server |
| **LLM (primary)** | Groq API ‚Äî Llama 4 Scout | Task decomposition, fast inference |
| **LLM (fallback 1)** | Google Gemini API | Automatic fallback if Groq unavailable |
| **LLM (fallback 2)** | Ollama (local) ‚Äî Llama 3.2 | Fully offline operation |
| **VLM** | Ollama ‚Äî Pixtral 12B / LLaVA | Keyframe visual analysis |
| **Orchestration** | LangGraph | Stateful AI pipeline with safety gates |
| **IK solver** | Custom FABRIK implementation | Joint angle computation from end-effector pos |

---

## Repository Structure

```
kinesysai/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ README.md                    ‚Üê You are here
‚îú‚îÄ‚îÄ üìÑ Makefile                     ‚Üê make dev / make install / make clean
‚îú‚îÄ‚îÄ üìÑ .env.example                 ‚Üê Environment variable template (no secrets)
‚îú‚îÄ‚îÄ üìÑ .gitignore
‚îÇ
‚îú‚îÄ‚îÄ üìÅ frontend/                    ‚Üê React + TypeScript UI
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ README.md                ‚Üê Frontend architecture & component guide
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ index.html
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ package.json
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ vite.config.ts
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ src/
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ README.md            ‚Üê Source code guide
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ App.tsx              ‚Üê Root component, layout, keyboard shortcuts
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ components/          ‚Üê All UI components (18 files)
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ engine/              ‚Üê IK solver, arm controller, physics
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ modes/               ‚Üê commandMode, teachMode, guideMode
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ services/            ‚Üê WebSocket, speech, TTS, MediaPipe
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ hooks/               ‚Üê useWebSocket
‚îÇ       ‚îî‚îÄ‚îÄ üìÅ game/                ‚Üê Puzzle engine, scoring, leaderboard
‚îÇ
‚îú‚îÄ‚îÄ üìÅ backend/                     ‚Üê Python FastAPI server
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ README.md                ‚Üê Backend architecture & API reference
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ app/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ main.py              ‚Üê FastAPI app, WebSocket handler, REST endpoints
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ ai/                      ‚Üê AI/ML layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ README.md            ‚Üê LLM/VLM client docs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ llm_client.py        ‚Üê Groq + Gemini + Ollama with fallback chain
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ vlm_client.py        ‚Üê Ollama vision model client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ task_decomposer.py   ‚Üê NL command ‚Üí action plan
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ scene_analyzer.py    ‚Üê Scene graph + spatial relations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ procedure_extractor.py ‚Üê VLM keyframe ‚Üí action sequence
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ prompts/             ‚Üê System prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core/                    ‚Üê Robot execution engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ README.md            ‚Üê Primitives & safety docs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ action_primitives.py ‚Üê 12 parameterized robot actions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ trajectory_planner.py ‚Üê Actions ‚Üí waypoint sequences
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ safety_validator.py  ‚Üê Hard constraint checking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ state_machine.py     ‚Üê LangGraph pipeline
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ trajectory_recorder.py ‚Üê Guide mode recording
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ demo/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ cached_responses.json ‚Üê Pre-cached demo responses
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ tests/                   ‚Üê 156 unit tests
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ test_primitives.py
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ test_safety.py
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ test_decomposer.py
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ test_state_machine.py
‚îÇ
‚îî‚îÄ‚îÄ üìÅ shared/
    ‚îú‚îÄ‚îÄ üìÑ README.md                ‚Üê Shared schema docs
    ‚îî‚îÄ‚îÄ üìÑ action_types.json        ‚Üê Canonical primitive definitions + schemas
```

---

## Quick Start

### Prerequisites

| Requirement | Version | Notes |
|-------------|---------|-------|
| Python | 3.11+ | Backend runtime |
| Node.js | 18+ | Frontend build |
| npm | 9+ | Package manager |
| Ollama | latest | Optional ‚Äî for local VLM / offline LLM |

### 1. Clone

```bash
git clone https://github.com/roshaninfordham/kinesysai.git
cd kinesysai
```

### 2. Configure environment

```bash
cp .env.example .env
```

Edit `.env` and add your API keys:

```env
# Get free at https://console.groq.com
GROQ_API_KEY=your_groq_api_key_here

# Get free at https://aistudio.google.com
GEMINI_API_KEY=your_gemini_api_key_here

# Optional ‚Äî local Ollama for offline use
OLLAMA_HOST=http://localhost:11434
```

> ‚ö†Ô∏è **Never commit `.env`** ‚Äî it is gitignored. Only `.env.example` is committed.

### 3. Install dependencies

```bash
make install
```

### 4. Start development servers

```bash
make dev
```

| Service | URL | Notes |
|---------|-----|-------|
| Frontend | http://localhost:5173 | React dev server with HMR |
| Backend | http://localhost:8000 | FastAPI + WebSocket |
| API Docs | http://localhost:8000/docs | Auto-generated Swagger UI |
| Health | http://localhost:8000/api/health | System status endpoint |

### Optional: Offline / Local LLM

```bash
# Install Ollama
brew install ollama   # macOS
# or: https://ollama.com/download

# Start Ollama service
ollama serve

# Pull models (text LLM + vision model)
ollama pull llama3.2:3b       # Lightweight text LLM (~2GB)
ollama pull llava:13b          # Vision model for Teach mode (~8GB)
```

With Ollama running and models pulled, KINESYS works **100% offline** ‚Äî no API keys needed.

---

## Keyboard Shortcuts

| Key | Action |
|-----|--------|
| `Space` | Push-to-talk (hold while speaking) |
| `R` | Reset scene |
| `1` | Switch to Command mode |
| `2` | Switch to Teach mode |
| `3` | Switch to Guide mode |
| `Esc` | Emergency stop |

---

## Action Primitives

12 parameterized robot actions, each with waypoint generation, safety validation, and TTS narration:

```
APPROACH   Move end-effector near a target
GRASP      Close gripper on a target object
RELEASE    Open gripper to release held object
TRANSLATE  Move to absolute or relative position
ROTATE     Rotate around X/Y/Z axis by N degrees
PLACE      Place held object at position or on target
PUSH       Push an object in a direction
POUR       Tilt a container to pour contents
STACK      Place held object on top of another
SORT       Arrange multiple objects by criterion
INSPECT    Move to observe a target (VLM capture)
WAIT       Pause for duration or condition
```

Full parameter schemas are in [`shared/action_types.json`](shared/action_types.json).

---

## Testing

```bash
cd backend
python3 -m pytest tests/ -v
```

**156 tests** across 4 test suites:

| Suite | Tests | Coverage |
|-------|-------|---------|
| `test_primitives.py` | 40 | All 12 action primitives, waypoint generation, per-primitive validation |
| `test_safety.py` | 36 | Workspace bounds, velocity limits, table collision, obstacle clearance |
| `test_decomposer.py` | 40 | LLM prompt construction, JSON parsing, retry logic |
| `test_state_machine.py` | 40 | LangGraph nodes, HITL gate, ERROR recovery, full pipeline |

---

## LLM Fallback Chain

KINESYS works even when API services are unavailable:

```
Voice command received
        ‚îÇ
        ‚ñº
   GROQ_API_KEY set?  ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∫ Try Groq (Llama 4 Scout)
        ‚îÇ                           ‚îÇ rate limit / error?
        ‚îÇ                           ‚ñº
        ‚îÇ                      Try Groq fallback model
        ‚îÇ                           ‚îÇ still failing?
        ‚îÇ                           ‚ñº
        NO                     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
  GEMINI_API_KEY set?  ‚îÄYES‚îÄ‚îÄ‚ñ∫ Try Gemini (gemini-2.5-flash-lite)
        ‚îÇ                           ‚îÇ error?
        NO                          ‚ñº
        ‚îÇ                      falling back...
        ‚îÇ                           ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
           Ollama running?  ‚îÄYES‚îÄ‚îÄ‚ñ∫ Try Ollama (llama3.2:3b)
                    ‚îÇ
                    NO
                    ‚îÇ
                    ‚ñº
           RuntimeError with diagnostic message
           (lists which keys are missing / services down)
```

---

## Gamification

KINESYS includes a built-in puzzle challenge system:

- **Puzzle Library** ‚Äî pre-defined robotic manipulation challenges (stack by color, sort by size, mirror arrangement, etc.)
- **Live Timer** ‚Äî countdown per puzzle
- **Scoring** ‚Äî time bonus + efficiency score based on action count
- **Leaderboard** ‚Äî persistent high-score board per puzzle

---

## Contributing

1. Fork the repo
2. Create a feature branch: `git checkout -b feature/your-feature`
3. Run tests: `cd backend && python3 -m pytest tests/ -v`
4. Commit with a descriptive message
5. Open a pull request

---

## License

MIT ‚Äî built for the Open Build Manhattan hackathon. Free to use, modify, and distribute.
